{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this utility file I will have all kinds of model that I need include inside and perform each as a baseline for the data I have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class, y_class = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.2, random_state=42)\n",
    "\n",
    "X_reg, y_reg = make_regression(n_samples=1000, n_features=20, n_informative=10, noise=0.2, random_state=42)\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "def tryRandomForest(reg = True, n_estimators=100, X_train = None, y_train = None, X_test = None, y_test = None):\n",
    "\n",
    "    if(reg):\n",
    "        # Initialize the RandomForestRegressor\n",
    "        rf_regressor = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "\n",
    "        # Train the model\n",
    "        rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        predictions = rf_regressor.predict(X_test)\n",
    "\n",
    "        # Calculate the mean squared error of the model\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "        print(f'Mean Squared Error of the RandomForest regression model: {mse:.2f}')\n",
    "    else:\n",
    "        # Initialize the RandomForestClassifier\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "        # Train the model\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        predictions = rf.predict(X_test)\n",
    "\n",
    "        # Calculate the accuracy of the model\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "        print(f'Accuracy of the RandomForest model: {accuracy:.2f}')\n",
    "\n",
    "# tryRandomForest(X_train=X_train_reg, y_train=y_train_reg, X_test=X_test_reg, y_test=y_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression MSE: 6058.55\n"
     ]
    }
   ],
   "source": [
    "# Xgboost\n",
    "\n",
    "def tryXgboost(objective = 'reg:squarederror', n_estimators=100, X_train = None, y_train = None, X_test = None, y_test = None, seed = 123):\n",
    "        \n",
    "\n",
    "        if(objective == 'reg:squarederror'):\n",
    "                xgb_reg = xgb.XGBRegressor(objective=objective, n_estimators=100, seed=42)\n",
    "                # Train the model\n",
    "                xgb_reg.fit(X_train, y_train)\n",
    "                # Predict labels for the test set\n",
    "                predictions = xgb_reg.predict(X_test)\n",
    "                # Evaluate the accuracy\n",
    "                mse = mean_squared_error(y_test, predictions)\n",
    "                print(f\"Regression MSE: {mse:.2f}\")\n",
    "        else:\n",
    "                xgb_clf = xgb.XGBClassifier(objective=objective, n_estimators=n_estimators, seed=seed)\n",
    "                # Train the model\n",
    "                xgb_clf.fit(X_train, y_train)\n",
    "                # Predict labels for the test set\n",
    "                predictions = xgb_clf.predict(X_test)\n",
    "                accuracy = accuracy_score(y_test, predictions)\n",
    "                print(f\"Multi-Class Classification Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "tryXgboost(X_train=X_train_reg, y_train=y_train_reg, X_test=X_test_reg, y_test=y_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy of linear: 0.81\n",
      "Classification Accuracy of poly: 0.91\n",
      "Classification Accuracy of rbf: 0.94\n",
      "Classification Accuracy of sigmoid: 0.68\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "# classification only\n",
    "\n",
    "def trySVM(kernel = 'linear', random_state=42, X_train = None, y_train = None, X_test = None, y_test = None):\n",
    "\n",
    "    svm_kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    for kernel in svm_kernels:\n",
    "    \n",
    "        svm_classifier = SVC(kernel=kernel, random_state=42)\n",
    "\n",
    "        # Train the SVM classifier\n",
    "        svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Predict labels for the test set\n",
    "        predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "        # Evaluate the accuracy\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(f\"Classification Accuracy of {kernel}: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "trySVM(X_train=X_train_class, y_train=y_train_class, X_test=X_test_class, y_test=y_test_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m3/5wh_b8lj1bv7zvdx4k22k7lw0000gn/T/ipykernel_8588/36412216.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "/var/folders/m3/5wh_b8lj1bv7zvdx4k22k7lw0000gn/T/ipykernel_8588/36412216.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float if mode == 'regression' else torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 0.0634, Accuracy: 97.88%\n",
      "Training Regressor:\n",
      "Epoch 30, Loss: 37696.1166, MSE: 37400.34\n"
     ]
    }
   ],
   "source": [
    "# Simple Transformer\n",
    "\n",
    "class SimpleTransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, mode='classification'):\n",
    "        super(SimpleTransformerModel, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.encoder = nn.Linear(input_size, 128)\n",
    "        self.transformer_block = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "        self.regressor = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.unsqueeze(1)  # Add a sequence dimension\n",
    "        x = self.transformer_block(x)\n",
    "        x = x.squeeze(1)\n",
    "        if self.mode == 'classification':\n",
    "            return self.classifier(x)\n",
    "        elif self.mode == 'regression':\n",
    "            return self.regressor(x)\n",
    "\n",
    "def prepare_data(mode, X_train,y_train, X_test, y_test):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float if mode == 'regression' else torch.long)\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "# Model Initialization\n",
    "model_class = SimpleTransformerModel(input_size=20, num_classes=2, mode='classification')\n",
    "model_reg = SimpleTransformerModel(input_size=20, num_classes=1, mode='regression')\n",
    "\n",
    "# Optimizers\n",
    "optimizer_class = optim.Adam(model_class.parameters(), lr=0.001)\n",
    "optimizer_reg = optim.Adam(model_reg.parameters(), lr=0.001)\n",
    "\n",
    "# Loss Functions\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_reg = nn.MSELoss()\n",
    "\n",
    "def train_model(model, data_loader, optimizer, criterion, epochs, mode):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        sum_squared_error = 0.0\n",
    "\n",
    "        for inputs, targets in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            if mode == 'classification':\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_correct += (predicted == targets).sum().item()\n",
    "                total_samples += targets.size(0)\n",
    "            elif mode == 'regression':\n",
    "                # Calculate sum of squared errors for MSE calculation\n",
    "                sum_squared_error += ((outputs.squeeze() - targets) ** 2).sum().item()\n",
    "                total_samples += targets.size(0)\n",
    "\n",
    "        epoch_loss = total_loss / total_samples\n",
    "        if mode == 'classification':\n",
    "            accuracy = total_correct / total_samples * 100\n",
    "        elif mode == 'regression':\n",
    "            mse = sum_squared_error / total_samples\n",
    "    if mode == 'classification':\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    elif mode == 'regression':\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, MSE: {mse:.2f}\")\n",
    "\n",
    "\n",
    "# Initialize and train classification model\n",
    "print(\"Training Classifier:\")\n",
    "class_loader = prepare_data('classification', X_train_class, y_train_class, X_test_class, y_test_class)\n",
    "train_model(model_class, class_loader, optimizer_class, criterion_class, 30, 'classification')\n",
    "\n",
    "print(\"Training Regressor:\")\n",
    "reg_loader = prepare_data('regression', X_train_reg, y_train_reg, X_test_reg, y_test_reg)\n",
    "train_model(model_reg, reg_loader, optimizer_reg, criterion_reg, 30, 'regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataFest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
